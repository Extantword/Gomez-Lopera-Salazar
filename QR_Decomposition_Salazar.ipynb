{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f6e835c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd0ceb",
   "metadata": {},
   "source": [
    "# Algoritmo QR\n",
    "\n",
    "Sea $A$ una matriz $n \\times n$. Supongamos que $A$ es no singular, y que sus autovalores cumplen que $|\\lambda_1| > |\\lambda_2| > ... > |\\lambda_n|$. Nuestro objetivo es encontrar los autovalores de $A$. Notemos que si encontramos una matriz $\\tilde{A}$ triangular superior, tal que $\\tilde{A}$ es similar a $A$, entonces hemos acabado, pues $\\tilde{A}$ tiene los mismos autovalores de $A$, y como $\\tilde{A}$ es triangular superior, sus autovalores aparecen en la diagonal, ya que el  polínomio característico es $p(x) = (x - a_1)(x - a_2)...(x - a_n)$, con los $a_i$ los elementos de la diagonal. Con base en lo anterior, enfoquemos nuestros esfuerzos en encontrar una aproximación a una matriz diagonal $\\tilde{A}$ similar a $A$. \n",
    "\n",
    "Sean $v_1, ..., v_n$ los autovectores de $A$ ordenados con respecto al orden decreciente de sus autovalores, y sea $T_k  = gen(v_1, ... v_k)$.  Supongamos que $Q = [Q_1Q_2]$ es una matriz ortogonal cuyas primeras $k$ columnas $(Q_1)$ forman una base ortogonal para el subespacio $T_k$. Entonces\n",
    "\n",
    "$$Q^TAQ =\n",
    "\\begin{bmatrix}\n",
    "Q_1^TAQ_1 & Q_1^TAQ_2 \\\\\n",
    "Q_2^TAQ_1 & Q_2^TAQ_2\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "A_{11} & A_{12} \\\\\n",
    "0 & A_{22}\n",
    "\\end{bmatrix}, \\quad (1)$$\n",
    "\n",
    "donde $Q_2^{T}AQ_1 = 0$ pues $T$ es invariante. La matriz anterior es una matriz triangular superior por bloques. Notemos que si $k = 1$, entonces el bloque $A_{11}$ tiene dimensión $1 \\times 1$. Se sigue que la primera columna tiene ceros excepto en la primera entrada de $Q^T A Q$, la cual es un autovalor de $Q^T A Q$, y por similaridad es un autovalor de $A$.  Si  se cumple simultáneamente que la primera columna forma una base ortogonal para $gen(v_1)$, y que las dos primeras columnas forman una base ortogonal para $gen(v_1, v_2)$, (esto es, $k = 2$), entonces la primera columna tiene ceros excepto en la primera entrada, y la segunda columna tiene ceros excepto en la primera y segunda entrada, pues se sigue respetando la forma $(1)$, donde el bloque $A_{11}$ tiene tamaño $2 \\times 2$, y por lo tanto el bloque inferior es de ceros. Si se cumplieran simultáneamente los casos $k = 1$, $k = 2$, ... $k = n$, entonces la matriz $Q^TAQ$ sería una matriz triangular superior, pues la forma $(1)$ se está cumpliendo simultáneamente para cada $k$.  Por lo tanto, hemos reducido el problema de encontrar una matriz triangular superior $\\tilde{A}$ similar a $A$, a encontrar una matriz ortogonal $Q$ tal que las primeras $k$ columnas formen una base para $gen(v_1, ..., v_k)$, $1 \\leq k \\leq n$. \n",
    "\n",
    "Recordemos que el método de potencias consiste en escoger un vector $v$ y aplicarle $A$ repetidamente para formar la sucesión\n",
    "\n",
    "$$v, Av, A^2v, A^3v, ...$$\n",
    "\n",
    "Asumiendo que se reescala adecuada el vector $v$ después de cada iteración, la sucesión usualmente convergerá a un autovector de $A$. Veamos rapidamente por qué.  Si $A$ tiene autovalores $\\lambda_1$, $\\lambda_2$, ..., $\\lambda_n$ tales que $|\\lambda_1| > |\\lambda_2| \\geq ... \\geq |\\lambda_n|$ y $n$ autovectores linealmente independientes $v_1, v_2, ..., v_n$ correspondientes a los autovalores anteriores, entonces podemos expresar $v$ como\n",
    "\n",
    "$$v = c_1v_1 + c_2v_2 + ... + c_nv_n.$$\n",
    "\n",
    "Luego, al aplicar $A$ repetidamente obtenemos\n",
    "\n",
    "$$A^mv = c_1\\lambda_1^mv_1 + c_2\\lambda_2^mv_2 + ... + c_n\\lambda_n^mv_n.$$\n",
    "\n",
    "Como $\\lambda_1$ domina a los demás autovalores, la componente en la dirección de $v_1$ eventualmente domina a las demás componentes, y por lo tanto $A^mv$ converge a un elemento del espacio $<v>$ cuando $m \\rightarrow \\infty$. \n",
    "\n",
    "Notemos que podemos ver el proceso anterior como como una iteración sobre _subespacios_: empezamos con un subespacio inicial $S =<v>$. Entonces iteramos\n",
    "\n",
    "$$S, AS, A^2S, A^3S, ...$$\n",
    "\n",
    "y la sucesión anterior \"converge\" al autoespacio $T = gen(v_1)$. Para volver rigurosa la noción anterior, definamos una métrica en el conjunto de supespacios $k$ - dimensionales de $\\mathbb{R}^n$: \n",
    "\n",
    "$$d(S, T) = sup_{s \\in S} inf_{t \\in T} ||s - t ||_2,$$\n",
    "    \n",
    "donde $||\\cdot|||_2$ es la norma Euclideana. El resultado principal de la convergencia de la iteración de subespacios es el siguiente:\n",
    "\n",
    "*Teorema 1*\n",
    "Sea $v_1$, ..., $v_n$ una base de autovectores de $A$. Sean $T = <v_1, ..., v_k>$, $U = <v_{k+1}, ..., v_n>$. Los espacios $T$ y $U$ son llamados _dominante_ y _co-dominante_, respectivamente. Sea $S$ un espacio $k$-dimensional de $\\mathbb{R}^n$ tal que $S \\cap U = (0)$. Entonces existe una constante $C$ tal que\n",
    "\n",
    "$$d(A^mS, T) \\leq C|\\lambda_{k+1}/\\lambda_{k}|^m$$\n",
    "\n",
    "para todo $m$. Por lo tanto $A^mS \\rightarrow T$ linealmente con tasa $|\\lambda_{k+1}/\\lambda_k|$.\n",
    "\n",
    "Sea $q_1^{0}, ..., q_n^{0}$ una base para $S$. Entonces $A^m(q_{1}^0), ..., A^m(q_n^{0})$ es una base para $A^mS$, ya que como $A^m$ es invertible, en cada iteración seguimos obteniendo un conjunto generador y linealmente independiente. Por el teorema anterior, podemos iterar la base $q_1^{0}, ..., q_n^{0}$ con $A$ para obtener una base para $T$. Si después de cada iteración ortogonalizamos la base obtenida de izquierda a derecha, entonces la matriz $Q_m$ cuyas columnas son  $A^m(q_{1}^0), ..., A^m(q_n^{0})$ sería una matriz ortogonal cuyas columnas forman una base para un espacio muy cercano a $T$. Más aún, la ortogonalización preserva la propiedad de que los primeros $k$ vectores forman una base para $gen(A^m(q_{1}^0), ..., A^m(q_k^{0})$ para cada $k$, y por lo tanto $Q_m$ converge a una matriz $Q$ ortogonal cuyas $k$ primeras columnas forman una base de $T_k = gen(v_1, ..., v_k)$ para cada $k$, que es precisamente lo que estamos buscando. \n",
    "\n",
    "Por lo tanto, ahora tenemos una idea para obtener el $Q$ buscado: empezamos con una base ortogonal $e_1, ..., e_n$ del espacio, y formamos una matriz $Q_0$ cuyas columnas son los elementos de esta base. A continuación calculamos $AQ_0$, y buscamos una nueva matriz $Q_1$ cuyas columnas sean la ortogonalización de las matrices de $A$. Esto es, buscamos una matriz $Q_1$ y una matriz $R_1$ tal que $A = Q_1R_1$. A continuación hacemos $AQ_1$, y buscamos matrices $Q_2R_2$ tales que $AQ_1 = Q_2R_2$. En general, definimos \n",
    "\n",
    "$$D_m = AQ_{m-1}$$,\n",
    "\n",
    " y buscamos matrices $Q_m$, $R_m$ tal que $D_m = Q_{m+1}R_{m+1}$. Verificamos la convergencia calculando $Q^{T}_mAQ_m$, y verificando que los elementos de la triangular inferior estén cerca de $0$. Una vez nos hemos acercado lo suficiente a una matriz triangular superior, recolectamos los valores en la diagonal, que corresponden a los autovalores de $A$.  Este es el algoritmo $QR$. \n",
    "\n",
    "Podríamos ortogonalizar usando el proceso de Graham-Schmidt. Sin embargo, en la práctica el algoritmo no es estable numericamente. Por lo tanto usaremos matrices de Householder.\n",
    "\n",
    "## Descomposición QR\n",
    "\n",
    "Sea $A$ una matriz real cuadrada. Entonces $A$ puede ser factorizada como \n",
    "\n",
    "$$ A = QR, $$\n",
    "\n",
    "donde $Q$ es una matriz ortogonal y $R$ es una matriz triangular superior. A continuación, describiremos un método para hallar la descomposición QR de cualquier matriz cuadrada A. Sea $v \\in \\mathbb{R}^n$ un vector no nulo. Sea $H$ una matriz $n \\times n$ de la forma\n",
    "\n",
    "$$ H = I - 2vv^{T}.$$\n",
    "\n",
    "Entonces $H$ es llamada una _matriz de Householder_. Llamamos a $v$ un vector de Householder. Si aplicamos $H$ a un vector $x$, este es reflejado en el hiperplano $gen(v)^{\\bot}$. Es directo verificar que las matrices de Householder son simétricas y ortogonales. La utilidad de las matrices de Householder para la descomposición QR yace en el siguiente hecho: sea $x \\in R^{n}$, y sea $u = x - s||x||e_{1}$. Si tomamos $v = u / ||u||$ la normalización de $u$, entonces\n",
    "\n",
    "$$Hx = (I - 2\\frac{u u^{T}}{u^{T}u})x = ||x||e_{1}$$. \n",
    "\n",
    "Esto es, aplicar $H$ a $x$ lo convierte en un múltiplo de $e_{1}$, con la misma magnitud de $x$. Otra forma de verlo es que hemos dejado en $0$ todas las componentes de $x$ excepto por la primera, en la que ha quedado su magnitud. Se sigue que si tomamos una sucesión adecuada de matrices de Householder $H_1, H_2, ..., H_{n-1}$ podemos convertir una matriz $A$ en una matriz triangular superior, ya que podemos ir convirtiendo en $0$ los elementos debajo de la diagonal. Veamos como podemos hacerlo con un ejemplo. Definamos una matriz A de tamaño $4 \\times 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "70e5a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 0, 0, 0],\n",
    "              [3, 2, 0, 0], \n",
    "              [1, 1, 4, 0],\n",
    "              [1, 2, 1, 7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f5330",
   "metadata": {},
   "source": [
    "Ahora, definamos $x$ como la primera columna de la matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eedaaee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 1, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = A[:, 0]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189510f",
   "metadata": {},
   "source": [
    "La norma de $x$ es: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "46faf8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4641016151377544"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c547fb",
   "metadata": {},
   "source": [
    "Calculamos $u$, $v$ y $H$ como lo describimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "457ea81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.289,  0.866,  0.289,  0.289],\n",
       "       [ 0.866, -0.054, -0.351, -0.351],\n",
       "       [ 0.289, -0.351,  0.883, -0.117],\n",
       "       [ 0.289, -0.351, -0.117,  0.883]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = x - np.linalg.norm(x) * np.eye(1, 4, 0)\n",
    "v = (1/np.linalg.norm(u)) *  u\n",
    "H1 = np.eye(4) - 2*v*v.T\n",
    "H1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff89474",
   "metadata": {},
   "source": [
    "La matriz anterior es la matriz de Householder $H_1$. Notemos que si aplicamos $H_1$ a $x$ obtenemos el siguiente arreglo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "efbe18ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.464, -0.   ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286f2ae",
   "metadata": {},
   "source": [
    "Lo que nos indica que la matriz $H$ efectivamente hace lo que queremos: dejar en $0$ todas las componentes de $x$ excepto por la primera, en la que queda su magnitud. Además, el producto $HA$ es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8bff77bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.464, -1.732,  1.443,  2.021],\n",
       "       [-0.   , -0.891, -1.757, -2.46 ],\n",
       "       [ 0.   ,  1.703,  3.414, -0.82 ],\n",
       "       [-0.   ,  2.703,  0.414,  6.18 ]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1.dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c3a89",
   "metadata": {},
   "source": [
    "Es decir, hemos logrado poner los ceros que queríamos en la primera columna de A. Ahora, queremos poner ceros en la siguiente columna. Esto es, queremos hacerle el proceso anterior al vector $x = \\begin{bmatrix}\n",
    "{-2.029} \\\\\n",
    "{-0.343} \\\\\n",
    "{0.657} \n",
    "\\end{bmatrix}$. Notemos que este vector ahora tiene $3$ componentes, pues lo que queremos es tranformar a $A$ en una matriz triangular superior, por lo que no nos interesa lo que le suceda a la primera componente. Luego, debemos operar ahora sobre la submatriz $(n - 1) \\times (n - 1)$ que resulta de omitir la primera fila y la segunda columna. En nuestro ejemplo, ahora queremos operar sobre la siguiente matriz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2bf6b825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.891, -1.757, -2.46 ],\n",
       "       [ 1.703,  3.414, -0.82 ],\n",
       "       [ 2.703,  0.414,  6.18 ]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_matrix1 = H1.dot(A)[1:, 1:]\n",
    "sub_matrix1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5db15",
   "metadata": {},
   "source": [
    "Como habíamos hecho antes, ahora definamos $x_2$ como la primera columna de la matriz anterior, y calculemos la matriz $H_2$ correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "47956f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.269,  0.513,  0.815],\n",
       "       [ 0.513,  0.792, -0.33 ],\n",
       "       [ 0.815, -0.33 ,  0.477]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2  = sub_matrix1[:, 0]\n",
    "u2 = x2 - np.linalg.norm(x2) * np.eye(1, 3, 0) \n",
    "v2 = (1/np.linalg.norm(u2)) *  u2\n",
    "H2 = np.eye(3) - 2*v2*v2.T\n",
    "H2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eafa0a",
   "metadata": {},
   "source": [
    "Recordemos que las matrices $A$ y $H_1A$ son matrices $4 \\times 4$. Sin embargo nuestra $H_2$ actual es una matriz $3 \\times 3$, pues esta en principio solo debía operar en una submatriz de $HA$. Arreglamos este problema introduciendo un padding a la matriz $H_2$, de tal manera que la primera fila y la primera columna de $H_2$ sean el vector $e_1$. Esto no afecta la ortogonalidad ni la simetría de $H_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b6561ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   , -0.269,  0.513,  0.815],\n",
       "       [ 0.   ,  0.513,  0.792, -0.33 ],\n",
       "       [ 0.   ,  0.815, -0.33 ,  0.477]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = np.array([1, 0, 0, 0])\n",
    "\n",
    "H_padded = np.zeros((4, 4))\n",
    "\n",
    "H_padded[0, :] = e1  \n",
    "H_padded[:, 0] = e1  \n",
    "\n",
    "H_padded[1:, 1:] = H2\n",
    "\n",
    "H2 = H_padded\n",
    "H2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ced6a0",
   "metadata": {},
   "source": [
    "Ahora calculamos $H_2H_1A$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1a2ee8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.464, -1.732,  1.443,  2.021],\n",
       "       [-0.   ,  3.317,  2.563,  5.276],\n",
       "       [ 0.   ,  0.   ,  1.666, -3.951],\n",
       "       [-0.   ,  0.   , -2.361,  1.21 ]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2.dot(H1).dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb22f93",
   "metadata": {},
   "source": [
    "Siguiendo el mismo proceso anterior, obtenemos una matriz $H_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2b717b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  1.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.577, -0.817],\n",
       "       [ 0.   ,  0.   , -0.817, -0.577]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_matrix2 = H2.dot(H1).dot(A)[2:, 2:]\n",
    "x3  = sub_matrix2[:, 0]\n",
    "u3 = x3 - np.linalg.norm(x3) * np.eye(1, 2, 0) \n",
    "v3 = (1/np.linalg.norm(u3)) *  u3\n",
    "H3 = np.eye(2) - 2*v3*v3.T\n",
    "\n",
    "#Le hacemos el padding\n",
    "e1 = np.array([1, 0, 0, 0])\n",
    "e2 = np.array([0, 1, 0, 0])\n",
    "\n",
    "H_padded = np.zeros((4, 4))\n",
    "\n",
    "H_padded[0, :] = e1  \n",
    "H_padded[:, 0] = e1  \n",
    "\n",
    "H_padded[1, :] = e2  \n",
    "H_padded[:, 1] = e2  \n",
    "\n",
    "H_padded[2:, 2:] = H3\n",
    "\n",
    "H3 = H_padded\n",
    "H3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0725e",
   "metadata": {},
   "source": [
    "Notemos que por construcción $H_3H_2H_1A$ debe ser una matriz triangular superior $R$. En efecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8806c355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.464, -1.732,  1.443,  2.021],\n",
       "       [-0.   ,  3.317,  2.563,  5.276],\n",
       "       [ 0.   , -0.   ,  2.889, -3.267],\n",
       "       [ 0.   , -0.   ,  0.   ,  2.53 ]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = H3.dot(H2).dot(H1).dot(A)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a216a",
   "metadata": {},
   "source": [
    "Como $H_1$, $H_2$ y $H_3$ son ortonogales, si definimos $Q = (H_3H_2H_1)^T$, entonces $Q = H_1^{T}H_2^{T}H_{3}^T = H_1^{-1}H_2^{-1}H_3^{-1}$, y por lo tanto $Q(H_3H_2H_1A) = QR = A$. Esto es, hemos encontrado la descomposición $QR$ de $A$. Lo comprobamos en el ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "47575580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz A: \n",
      "[[ 1  0  0  0]\n",
      " [ 3 -3  0  0]\n",
      " [ 1  1  4  0]\n",
      " [ 1  2  1  7]]\n",
      "\n",
      "Matriz QR: \n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 3. -3.  0.  0.]\n",
      " [ 1.  1.  4. -0.]\n",
      " [ 1.  2.  1.  7.]]\n"
     ]
    }
   ],
   "source": [
    "Q = (H3.dot(H2).dot(H1)).T\n",
    "print('Matriz A: ')\n",
    "print(A)\n",
    "\n",
    "print('\\nMatriz QR: ')\n",
    "print(Q.dot(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93ebe6",
   "metadata": {},
   "source": [
    "El anterior era solo un ejemplo ilustrativo. En la práctica se realizan cosas extras para llegar más eficientemente a la descomposición. Por ejemplo, para almacenar las matrices de Householder $H_i$ no necesitamos guardar toda la matriz, sino solo el vector de Householder $v$. \n",
    "\n",
    "Recordemos que nuestro objetivo es encontrar los autovalores de $A$. A continuación, realizaremos unas observaciones que nos llevaran intuitivamente (pero también rigurosamente) al algoritmo QR. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e679d",
   "metadata": {},
   "source": [
    "# Análisis del error\n",
    "\n",
    "Por el teorema de la convergencia de subespacios, $A^mS$ converge a T con tasa $C|\\lambda_{k+1} / \\lambda_k|$. Se sigue que el algoritmo converge a una matriz triangular superior a esta tasa. Es posible modificar el algoritmo (con shifts) para lograr una convergencia mucho más rápida a los autovalores.  \n",
    "\n",
    "# Implementación \n",
    "\n",
    "A continuación presentamos implementaciones para calcular la descomposición $QR$, y otro para calcular la matriz triangular superior $\\tilde{A}$ que permite recolectar los autovalores de $A$.  \n",
    "### householder_qr()\n",
    "\n",
    "Es una implementación generalizada del proceso anterior para encontrar la descomposición $QR$ a través de matrices de Householder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b8d5efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def householder_qr(A):\n",
    "    \"\"\"\"\n",
    "    \n",
    "    Parametros\n",
    "    A: Matriz a descomponer\n",
    "    \n",
    "    Returna:\n",
    "    tupla: (Q, R) donde Q es matriz ortogonal y R es matriz triangular superior\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.array(A, dtype=float)\n",
    "    m, n = A.shape\n",
    "    Q = np.eye(m)\n",
    "    R = A.copy()\n",
    "    \n",
    "    # Itera cada columna\n",
    "    for j in range(min(m-1, n)):\n",
    "        # Extraemos la columna en la que estamos trbajando\n",
    "        x = R[j:, j]\n",
    "        \n",
    "        # Calculamos el vector de householder\n",
    "        e1 = np.zeros_like(x)\n",
    "        e1[0] = 1\n",
    "        \n",
    "        alpha = -np.sign(x[0]) * np.linalg.norm(x) # Multiplicamos por el signo para estabilidad\n",
    "                                                   # numérica\n",
    "        u = x - alpha * e1\n",
    "        v = u / np.linalg.norm(u)\n",
    "        H = np.eye(m)\n",
    "        H[j:, j:] -= 2.0 * np.outer(v, v)\n",
    "        R = H @ R\n",
    "        Q = Q @ H.T # Vamos acumulando Q\n",
    "    \n",
    "    # Ponemos ceros en la triangular inferior para asegurar que es triangular superior\n",
    "    for i in range(m):\n",
    "        for j in range(i):\n",
    "            if j < n:\n",
    "                R[i, j] = 0.0\n",
    "    \n",
    "    return Q, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b465b756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0],\n",
       "       [ 3, -3,  0,  0],\n",
       "       [ 1,  1,  4,  0],\n",
       "       [ 1,  2,  1,  7]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7181bc68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -0., -0., -0.],\n",
       "       [ 3., -3.,  0.,  0.],\n",
       "       [ 1.,  1.,  4., -0.],\n",
       "       [ 1.,  2.,  1.,  7.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, R = householder_qr(A)\n",
    "Q @ R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea0877",
   "metadata": {},
   "source": [
    "### qr_eigenvalue_algorithm()\n",
    "\n",
    "El siguiente código calcula el algoritmo $QR$, mediante el procedimiento descrito anteriormente. Además, se hace uso de la siguiente observación:\n",
    "\n",
    "Para $A_1 = A_1I$, esta tiene una descomposición\n",
    "\n",
    "$$A_1 = Q'_2R'_2.$$\n",
    "\n",
    "Luego $Q'^T_2A_1 = R'_2$. Por definición, $A_1 = Q_1^{T}AQ_1$,  $D_1 = AQ_1$, y $D_1$ tiene una descomposición $Q_2R_2$. Se sigue que\n",
    "\n",
    "$$Q_2R_2 = D_1 = AQ_1 = Q_1A_1Q_1^TQ_1 = Q_1A_1 = Q_1Q'_2R'_2.$$\n",
    "\n",
    "Ahora, es un hecho que la descomposición $QR$ es única si $A$ se escoge invertible, y $R$ con diagonal positiva. Por lo tanto $Q_2 = Q_1Q_2', R_2 = R_2'$ y así\n",
    "\n",
    "$$A_2 = Q_2^TAQ_2 = Q_2'^TQ_1^TAQ_1Q_2' = Q_2^TA_1Q'_2 = R_2'Q_2'. $$\n",
    "\n",
    "Ahora para calcular $A_3$, basta calcular la descomposición $Q_3R_3$ de $A_2$, y hacer $A_3 = R_3Q_3$. La siguiente implementación usa esta idea para calcular matrices triangulares $A_m$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "19ea4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_eigenvalue_algorithm(A, max_iter=1000, tolerance=1e-10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computa autovalores usando el algoritmo QR\n",
    "    \n",
    "    Parameters:\n",
    "    A (list): Matriz a la que se hallarán los autovalores\n",
    "    max_iter (int): Número máximo de iteraciones\n",
    "    tolerance (float): Tolerancia de la convergencia\n",
    "    \n",
    "    Returna:\n",
    "    np.array: eigenvalues\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.array(A, dtype=float)\n",
    "    n = A.shape[0]\n",
    "    V = np.eye(n)\n",
    "    H = A.copy()\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        \n",
    "        Q, R = householder_qr(H)\n",
    "        H = R @ Q\n",
    "        V = V @ Q\n",
    "        \n",
    "        off_diag_norm = np.sum(np.abs(np.tril(H, -1))) #Calcula norma de diagonal debajo de la diagonal principal.\n",
    "        \n",
    "        if off_diag_norm < tolerance: #Con base en el valor anterior, verificamos la convergencia del método\n",
    "            break\n",
    "    \n",
    "    eigenvalues = np.diag(H)\n",
    "    \n",
    "    return eigenvalues\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f3cdaaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  4., -3.,  1.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_eigenvalue_algorithm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863549f",
   "metadata": {},
   "source": [
    "# Ejemplo\n",
    "\n",
    "Calculamos los autovalores de las siguientes matrices dadas en la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f9e3f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 1], \n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[3, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "C = np.array([[2, 3],\n",
    "              [1, 4]])\n",
    "\n",
    "D = np.array([[1, 1, 2],\n",
    "              [2, 1, 1], \n",
    "              [1, 1, 3]])\n",
    "\n",
    "E = np.array([[1, 1, 2],\n",
    "              [2, 1, 3],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "F = np.array([[2, 1, 2], \n",
    "              [1, 1, 3],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "G = np.array([[1, 1, 1, 2],\n",
    "              [2, 1, 1, 1],\n",
    "              [3, 2, 1, 2],\n",
    "              [2, 1, 1, 4]])\n",
    "\n",
    "H = np.array([[1, 2, 1, 2],\n",
    "              [2, 1, 1, 1], \n",
    "              [3, 2, 1, 2],\n",
    "              [2, 1, 1, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c5567086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los autovalores de A son: [5. 1.]\n",
      "\n",
      "Los autovalores de B son: [6. 1.]\n",
      "\n",
      "Los autovalores de C son: [5. 1.]\n",
      "\n",
      "Los autovalores de D son: [ 4.507  0.778 -0.285]\n",
      "\n",
      "Los autovalores de E son: [ 4.049 -0.692 -0.357]\n",
      "\n",
      "Los autovalores de F son: [ 4.125 -0.762  0.637]\n",
      "\n",
      "Los autovalores de G son: [ 6.635  1.509 -0.736 -0.407]\n",
      "\n",
      "Los autovalores de H son: [ 6.827  1.728 -1.088 -0.467]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Los autovalores de A son: {qr_eigenvalue_algorithm(A)}\\n')\n",
    "print(f'Los autovalores de B son: {qr_eigenvalue_algorithm(B)}\\n')\n",
    "print(f'Los autovalores de C son: {qr_eigenvalue_algorithm(C)}\\n')\n",
    "print(f'Los autovalores de D son: {qr_eigenvalue_algorithm(D)}\\n')\n",
    "print(f'Los autovalores de E son: {qr_eigenvalue_algorithm(E)}\\n')\n",
    "print(f'Los autovalores de F son: {qr_eigenvalue_algorithm(F)}\\n')\n",
    "print(f'Los autovalores de G son: {qr_eigenvalue_algorithm(G)}\\n')\n",
    "print(f'Los autovalores de H son: {qr_eigenvalue_algorithm(H)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c097c1",
   "metadata": {},
   "source": [
    "Vemos que logramos obtener los autovalores de cada matriz. Las ventajas del método es que su implementación es sencilla, y que tenemos un resultado fuerte para la tasa de convergencia a una matriz triangular. El método presenta problemas cuando la matriz tiene autovalores con igual módulo, pues en este caso no converge a una matriz triangular superior, aunque igualmente se puede obtener información de los autovalores de la matriz. También el caso en el que el módulo de los autovalores es muy cercano presenta problemas. \n",
    "\n",
    "El caso en el que $A$ es no invertible no es tan patológico como parece: en una iteración, el autovalor $0$ se quita en la primera iteración, y el algoritmo sigue operando como si fuera una matriz no singular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bdc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
